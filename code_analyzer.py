"""
Agente analizador de c√≥digo especializado.
LLM dedicado a analizar archivos individuales en profundidad.
"""
import env_loader  # Cargar .env PRIMERO
import json
from typing import Dict, Any
from openai import OpenAI

from config import ANALYZER_MODEL, ANALYZER_SYSTEM_PROMPT, MAX_TOKENS_PER_FILE


class CodeAnalyzer:
    """LLM especializado en an√°lisis profundo de c√≥digo y documentaci√≥n."""
    
    def __init__(self, model: str = ANALYZER_MODEL):
        """
        Inicializa el analizador de c√≥digo.
        
        Args:
            model: Modelo de OpenAI a usar para an√°lisis
        """
        self.model = model
        self.client = OpenAI()
        self.system_prompt = ANALYZER_SYSTEM_PROMPT
    
    def _estimate_tokens(self, text: str) -> int:
        """Estima el n√∫mero de tokens en un texto (aproximado)."""
        # Aproximaci√≥n: 1 token ‚âà 4 caracteres
        return len(text) // 4
    
    def analyze_file(self, file_path: str, content: str, file_type: str) -> Dict[str, Any]:
        """
        Analiza un archivo de c√≥digo o documentaci√≥n.
        
        Args:
            file_path: Ruta del archivo
            content: Contenido del archivo
            file_type: Tipo de archivo (python, javascript, etc.)
            
        Returns:
            An√°lisis estructurado en formato JSON
        """
        print(f"üîç Analizando: {file_path}")
        
        # Verificar tama√±o
        tokens = self._estimate_tokens(content)
        if tokens > MAX_TOKENS_PER_FILE:
            print(f"‚ö†Ô∏è Archivo muy grande ({tokens} tokens). Tomando primeras l√≠neas...")
            # Tomar aproximadamente la mitad del l√≠mite
            content = content[:MAX_TOKENS_PER_FILE * 2]  # 2 chars ‚âà 1 token
        
        # Preparar prompt de an√°lisis
        user_prompt = f"""Analiza el siguiente archivo de c√≥digo/documentaci√≥n:

**Ruta del archivo:** {file_path}
**Tipo de archivo:** {file_type}

**Contenido:**
```{file_type}
{content}
```

Proporciona un an√°lisis completo en formato JSON siguiendo la estructura especificada en tu prompt del sistema.
"""
        
        try:
            # Llamar al LLM analizador
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,  # Baja temperatura para an√°lisis consistente
                response_format={"type": "json_object"}  # Forzar respuesta JSON
            )
            
            # Extraer y parsear respuesta
            analysis_text = response.choices[0].message.content
            analysis = json.loads(analysis_text)
            
            # Agregar metadata adicional
            analysis["file_path"] = file_path
            analysis["tokens_analyzed"] = tokens
            
            print(f"‚úÖ An√°lisis completado: {file_path}")
            return analysis
            
        except json.JSONDecodeError as e:
            print(f"‚ùå Error parseando JSON del an√°lisis: {e}")
            # Retornar an√°lisis b√°sico de fallback
            return {
                "file_path": file_path,
                "file_type": file_type,
                "summary": "Error: No se pudo parsear el an√°lisis",
                "error": str(e),
                "raw_response": analysis_text if 'analysis_text' in locals() else None
            }
        
        except Exception as e:
            print(f"‚ùå Error analizando archivo: {e}")
            return {
                "file_path": file_path,
                "file_type": file_type,
                "summary": f"Error durante el an√°lisis: {str(e)}",
                "error": str(e)
            }
    
    def analyze_batch(self, files: list[tuple[str, str, str]]) -> list[Dict[str, Any]]:
        """
        Analiza m√∫ltiples archivos en lote.
        
        Args:
            files: Lista de tuplas (file_path, content, file_type)
            
        Returns:
            Lista de an√°lisis
        """
        results = []
        total = len(files)
        
        for idx, (file_path, content, file_type) in enumerate(files, 1):
            print(f"\nüìä Progreso: {idx}/{total}")
            analysis = self.analyze_file(file_path, content, file_type)
            results.append(analysis)
        
        return results
    
    def quick_summary(self, file_path: str, content: str, file_type: str) -> str:
        """
        Genera un resumen r√°pido sin estructura JSON completa.
        
        Args:
            file_path: Ruta del archivo
            content: Contenido del archivo
            file_type: Tipo de archivo
            
        Returns:
            Resumen en texto plano
        """
        prompt = f"""Resume brevemente (2-3 oraciones) qu√© hace este archivo:

**Archivo:** {file_path}
**Tipo:** {file_type}

```{file_type}
{content[:5000]}
```

Responde solo con el resumen, sin formato adicional."""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Modelo m√°s r√°pido para res√∫menes
                messages=[
                    {"role": "system", "content": "Eres un experto en an√°lisis de c√≥digo. Proporciona res√∫menes concisos y precisos."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return f"Error generando resumen: {str(e)}"
